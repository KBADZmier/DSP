{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0828ae76-9b4f-43cf-b5e3-720ac0632bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kamil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kamil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Kamil\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dane wczytane pomyślnie. Liczba wiadomości: 5572\n",
      "\n",
      "Przykładowe wiadomości:\n",
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "\n",
      "Rozkład klas:\n",
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Przetwarzanie tekstu...\n",
      "\n",
      "Trenowanie modelu...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1448\n",
      "        spam       0.98      0.87      0.92       224\n",
      "\n",
      "    accuracy                           0.98      1672\n",
      "   macro avg       0.98      0.93      0.96      1672\n",
      "weighted avg       0.98      0.98      0.98      1672\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1444    4]\n",
      " [  29  195]]\n",
      "\n",
      "Accuracy: 0.9802631578947368\n",
      "\n",
      "Testowanie na przykładowych wiadomościach:\n",
      "\n",
      "Wiadomość: 'Congratulations! You've won a $1000 prize! Click here to claim!'\n",
      "Klasyfikacja: spam\n",
      "Prawdopodobieństwo: 1.00\n",
      "\n",
      "Wiadomość: 'Hi John, just reminding about our meeting tomorrow at 10am'\n",
      "Klasyfikacja: ham\n",
      "Prawdopodobieństwo: 1.00\n",
      "\n",
      "Wiadomość: 'URGENT: Your bank account has been compromised. Click to verify'\n",
      "Klasyfikacja: spam\n",
      "Prawdopodobieństwo: 0.91\n",
      "\n",
      "Wiadomość: 'The report you requested is attached. Best regards, Team'\n",
      "Klasyfikacja: ham\n",
      "Prawdopodobieństwo: 0.95\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Pobranie potrzebnych zasobów NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Funkcja do czyszczenia i preprocessing tekstu\n",
    "def preprocess_text(text):\n",
    "    # Usunięcie znaków interpunkcyjnych\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Konwersja na małe litery\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenizacja\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Usunięcie stopwords i lematyzacja\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Główna funkcja\n",
    "def main():\n",
    "    # Wczytanie danych z lokalnego pliku\n",
    "    try:\n",
    "        # Plik spam.csv powinien być w tym samym folderze co skrypt\n",
    "        df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "        # Wybieramy tylko potrzebne kolumny i zmieniamy ich nazwy\n",
    "        df = df[['v1', 'v2']].rename(columns={'v1': 'label', 'v2': 'text'})\n",
    "        print(\"Dane wczytane pomyślnie. Liczba wiadomości:\", len(df))\n",
    "    except Exception as e:\n",
    "        print(\"Błąd wczytywania pliku:\", e)\n",
    "        return\n",
    "\n",
    "    # Przykładowe wiadomości\n",
    "    print(\"\\nPrzykładowe wiadomości:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Rozkład klas\n",
    "    print(\"\\nRozkład klas:\")\n",
    "    print(df['label'].value_counts())\n",
    "\n",
    "    # Preprocessing tekstu\n",
    "    print(\"\\nPrzetwarzanie tekstu...\")\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "    # Podział danych na zbiór treningowy i testowy\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['cleaned_text'], \n",
    "        df['label'], \n",
    "        test_size=0.3, \n",
    "        random_state=42,\n",
    "        stratify=df['label']  # Zachowujemy proporcje klas\n",
    "    )\n",
    "\n",
    "    # Tworzenie modelu: TF-IDF + Naive Bayes\n",
    "    print(\"\\nTrenowanie modelu...\")\n",
    "    model = make_pipeline(\n",
    "        TfidfVectorizer(\n",
    "            max_features=5000,  # Ograniczamy do 5000 najważniejszych słów\n",
    "            ngram_range=(1, 2)  # Uwzględniamy również bigramy\n",
    "        ),\n",
    "        MultinomialNB(alpha=0.1)  # Dodałem parametr wygładzania\n",
    "    )\n",
    "\n",
    "    # Trenowanie modelu\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predykcja na zbiorze testowym\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Ocena modelu\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # Przykład użycia modelu do klasyfikacji nowej wiadomości\n",
    "    test_messages = [\n",
    "        \"Congratulations! You've won a $1000 prize! Click here to claim!\",\n",
    "        \"Hi John, just reminding about our meeting tomorrow at 10am\",\n",
    "        \"URGENT: Your bank account has been compromised. Click to verify\",\n",
    "        \"The report you requested is attached. Best regards, Team\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\nTestowanie na przykładowych wiadomościach:\")\n",
    "    for msg in test_messages:\n",
    "        cleaned_msg = preprocess_text(msg)\n",
    "        prediction = model.predict([cleaned_msg])[0]\n",
    "        print(f\"\\nWiadomość: '{msg}'\")\n",
    "        print(f\"Klasyfikacja: {prediction}\")\n",
    "        print(f\"Prawdopodobieństwo: {model.predict_proba([cleaned_msg]).max():.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70209d89-38ee-4d7a-b0b6-71c0d587d5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
